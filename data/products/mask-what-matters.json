{
  "slug": "mask-what-matters",
  "name": "Mask What Matters",
  "product_url": "",
  "description": "We study object hallucination in Multimodal Large Language Models (MLLMs) and improve visual contrastive decoding (VCD) by constructing an object-aligned auxiliary view. We leverage object-centric attention in self-supervised Vision Transformers. In particular, we remove the most salient visual evidence to construct an auxiliary view that disrupts unsupported tokens and produces a stronger contrast signal. Our method is prompt-agnostic, model-agnostic, and can be seamlessly plugged into the existing VCD pipeline with little computation overhead, i.e., a single cacheable forward pass. Empirically, our method demonstrates consistent gains on two popular object hallucination benchmarks across two MLLMs.",
  "product_type": "other",
  "category": "ai-creative-media",
  "status": "active",
  "meta": {
    "added_date": "2026-02-13",
    "last_updated": "2026-02-13",
    "provenance": {
      "name": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "description": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "product_type": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "category": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "status": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "company.name": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "company.url": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "sub_category": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "tags": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "key_people": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "release_date": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      }
    }
  },
  "sources": [
    {
      "url": "http://arxiv.org/abs/2602.11737v1",
      "source_name": "arxiv",
      "scraped_at": "2026-02-13"
    }
  ],
  "company": {
    "name": "Mask What Matters",
    "url": "https://www.bing.com/search?q=Mask+What+Matters+AI"
  },
  "sub_category": "image-generation",
  "tags": [
    "text-to-image",
    "creators",
    "diffusion-model",
    "computer-vision",
    "research",
    "multimodal",
    "content-creation",
    "nlp"
  ],
  "key_people": [
    {
      "name": "Boqi Chen",
      "title": "Author",
      "is_founder": false
    },
    {
      "name": "Xudong Liu",
      "title": "Author",
      "is_founder": false
    },
    {
      "name": "Jianing Qiu",
      "title": "Author",
      "is_founder": false
    }
  ],
  "release_date": "2026-02-12"
}
