{
  "slug": "lora-squeeze",
  "name": "LoRA-Squeeze",
  "product_url": "",
  "description": "Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment complexity of heterogeneous-rank modules and more sophisticated LoRA derivatives. In this work, we introduce LoRA-Squeeze, a simple and efficient methodology that aims to improve standard LoRA learning by changing LoRA module ranks either post-hoc or dynamically during training}. Our approach posits that it is better to first learn an expressive, higher-rank solution and then compress it, rather than learning a constrained, low-rank solution directly. The method involves fine-tuning with a deliberately high(er) source rank, reconstructing or efficiently approximating the reconstruction of the full weight update matrix, and then using Randomized Singular Value Decomposition (RSVD) to create a new, compressed LoRA module at a lower target rank. Extensive experiments across 13 text and 10 vision-language tasks show that post-hoc compression often produces lower-rank adapters that outperform those trained directly at the target rank, especially if a small number of fine-tuning steps at the target rank is allowed. Moreover, a gradual, in-tuning rank annealing variant of LoRA-Squeeze consistently achieves the best LoRA size-performance trade-off.",
  "product_type": "llm",
  "category": "ai-foundation-model",
  "status": "active",
  "meta": {
    "added_date": "2026-02-13",
    "last_updated": "2026-02-12",
    "provenance": {
      "name": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "description": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "product_type": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "category": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "status": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "company.name": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "company.url": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "sub_category": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "tags": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "key_people": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      },
      "release_date": {
        "source": "arxiv",
        "tier": 2,
        "confidence": 0.75,
        "updated_at": "2026-02-13"
      }
    }
  },
  "sources": [
    {
      "url": "http://arxiv.org/abs/2602.10993v1",
      "source_name": "arxiv",
      "scraped_at": "2026-02-13"
    }
  ],
  "company": {
    "name": "LoRA-Squeeze",
    "url": "https://www.bing.com/search?q=LoRA-Squeeze+AI"
  },
  "sub_category": "text-generation",
  "tags": [
    "nlp",
    "fine-tuning",
    "research",
    "chatbot",
    "researchers"
  ],
  "key_people": [
    {
      "name": "Ivan VuliÄ‡",
      "title": "Author",
      "is_founder": false
    },
    {
      "name": "Adam Grycner",
      "title": "Author",
      "is_founder": false
    },
    {
      "name": "Quentin de Laroussilhe",
      "title": "Author",
      "is_founder": false
    },
    {
      "name": "Jonas Pfeiffer",
      "title": "Author",
      "is_founder": false
    }
  ],
  "release_date": "2026-02-11"
}
